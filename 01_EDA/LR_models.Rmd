---
title: "AirBNB Madrid models for price prediction"
author: "Katsiaryna Zaitsava, Antonio Fernández Cáceres, Alvaro Simón Merino"
date: "`r format(Sys.Date(), '%d de %B de %Y')`"
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[L]{Fundamentos de Análisis de Datos}
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE,message = FALSE,warning = FALSE}
# aquí ponemos los paquetes que vayamos necesitando
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(tidyr)
library(ggplot2)
library(nortest)
library(dplyr)
library(stringr)
library(readr)
library(VIM)
library(corrplot)
library(psych)
library(ipred)
library(DMwR)
library(car)
library(ggfortify)
library("stringi")
library(gvlma)
library(nortest)
library(leaps)
library('glmnet')
library(MASS)
library(caret)
library('geosphere')
```

# Transformacion y selecion previa de variables
Dado que los datos de las variables *description* y *host_id* no contienen información útil para futuras investigaciones, los eliminamos del conjunto de datos para construir una regresión lineal.

```{r lr rem, echo=FALSE}
data_train <- read_csv("data_train_clean.csv")
data_train$description <- NULL
data_train$host_id <- NULL
```

También vemos que necesitamos agrupar los datos por variable *property_type*.
```{r lr pt, echo=FALSE}
data_train$property_type[grep("Shared room", data_train$property_type)] <- "Shared room"
data_train$property_type[grep("Private room", data_train$property_type)] <- "Private room"
data_train$property_type[grep("Room", data_train$property_type)] <- "Private room"
data_train$property_type[grep("Entire", data_train$property_type)] <- "Entire place"
data_train$property_type[grep("house", data_train$property_type)] <- "Entire place"
data_train$property_type[grep("Casa", data_train$property_type)] <- "Entire place"
data_train$property_type[grep("Hut", data_train$property_type)] <- "Other"
data_train$property_type[grep("Cave", data_train$property_type)] <- "Other"
data_train$property_type[grep("Camper/RV", data_train$property_type)] <- "Other"
data_train$property_type[grep("Floor", data_train$property_type)] <- "Other"
```


Después de la transformación, recibimos 4 tipos principales de propiedades *property_type*.

```{r rl pt f, echo=FALSE}
levels(as.factor(data_train$property_type))
```

Vemos que en resultado salen datos de *property_type* muy precidos a *room_type*.

```{r rl rt pt, echo=FALSE}
data_train %>%
  group_by(property_type) %>%
  count()

data_train %>%
  group_by(room_type) %>%
  count()
```

En este caso eleminamos variable  *property_type* de nuestro conjunto de datos.

```{r rl pt rm, echo=FALSE}
data_train$property_type <- NULL
```

Transformamos datos *room_type* y *neighbourhood_group_cleansed* para mejorar usado de dummies de estos variables.
```{r rl transf, echo=FALSE}
data_train$room_type[grep("Hotel room", data_train$room_type)] <- "Hotel"
data_train$room_type[grep("Private room", data_train$room_type)] <- "Private"
data_train$room_type[grep("Shared room", data_train$room_type)] <- "Shared"
data_train$neighbourhood_group_cleansed[grep("Puente de Vallecas", data_train$neighbourhood_group_cleansed)] <- "Puente_de_Vallecas"
```

A partir del análisis preliminar de los datos, llegamos a la conclusión de que la mejor transformación para la variable de precio sería la transformación logarítmica (Lambda = 0), que se confirma con los resultados del uso de Box-Cox transformaion.

```{r rl trans price, echo=FALSE}
BoxCoxTrans(data_train$price)
```

Sobre de la estadística descriptiva, quedó claro que las variables cuantitativas tienen diferentes dimensiones, lo que requiere su estandarización.

```{r rl trans num, echo=FALSE}
num <- c('latitude', 'longitude', 'accommodates', 'bedrooms', 'minimum_nights', 'availability_365', 'review_scores_rating', 'reviews_per_month', 'host_exp_days', 'last_review_days', 'host_listings_count')
data_train_num <- data_train[,num]
norm_num <- preProcess(data_train_num)
data_train_nn <- predict(norm_num, data_train_num)
data_train[,num] <- data_train_nn
```

Nuestros datos cuantitativos ahora están estandarizados.
```{r rl sum, echo=FALSE}
summary(data_train[,num])
```

# Modelos de regresión lineal múltiple (elecion "a mano")
Anteriormente hemos preseleccionado ciertas variables después de la limpieza y análisis inicial de nuestro conjunto de datos. Primero, construyamos una regresión lineal, donde la variable dependiente será el *price* y todas las demás variables seleccionadas para el análisis actuarán como variables independientes. Necesitamos esto para determinar más claramente la presencia de multicolinealidad, utilizando el methodo del VIF.

```{r fit1, echo=FALSE}
fit_1 <- lm(log(price) ~. -price_per_person, data = data_train)
summary(fit_1)
vif(fit_1)
```

Primero, eliminamos de la regresión las variables para las que el valor del indicador VIF es más de 10.

```{r fit2, echo=FALSE}
fit_2 <- lm(log(price) ~. -price_per_person -longitude -neighbourhood_group_cleansed, data = data_train)
summary(fit_2)

vif(fit_2)
```

También existe una estrecha relación lineal entre las variables *accommodates* y *bedrooms*, por lo que eliminamos la variable *bedrooms* del modelo. A su vez, los coeficientes de las variables (*host_listings_count*, *host_is_superhost*, *review_scores_rating*, *last_review_days*) no son significativos y no se ha identificado previamente la influencia de estas variables sobre la variable dependiente. Por lo tanto, también excluimos estas variables del modelo.

```{r fit3, echo=FALSE}
fit_3 <- lm(log(price) ~. -price_per_person -longitude -neighbourhood_group_cleansed -host_is_superhost -host_listings_count -bedrooms -last_review_days -review_scores_rating, data = data_train)
summary(fit_3)
vif(fit_3)
```
Ahora, verifiquemos si el modelo resultante cumple las condiciones básicas para construir una regresión lineal.

```{r fit3 sum, echo=FALSE}
anova(fit_3)
plot(fit_3$residuals)
summary(gvlma(fit_3))
autoplot(fit_3)
```

Vemos que no se siguen las reglas básicas.

Consideremos con más detalle la relación de variables *price* y *accommodates*.

```{r inter, echo=FALSE}
ggplot(data_train, aes(accommodates, log(price))) + 
  geom_point() + 
  geom_smooth(col = 'blue', method = lm)

ggplot(data_train, aes(accommodates, log(price))) + 
  geom_point() + 
  geom_smooth(aes(color=room_type), method = lm)
```

En los gráficos, vemos que la línea de regresión es significativamente diferente para diferentes tipos  de *room_type*, a lo de mas se ve claramente 3 grupos signicativos como Entire home-Hotel, Private, Shared. debemos considerar el efecto de la variable *accommodates* en relación con la variable *room_type*.
También mostraremos un gráfico de la distribución del precio medio en función del tipo de *room_type*.
```{r inter mean, echo=FALSE}
data_train %>% group_by(room_type) %>%
  summarize(mean_price = mean(price)) %>%
  ggplot(aes(reorder(room_type, mean_price), y = mean_price, label=sprintf("%0.2f",round(mean_price, digits = 2))))+ 
  geom_bar(stat = "identity", color = "navyblue", size = 1.5, fill = "deepskyblue")+coord_flip()+
  xlab("Room Type")+
  ylab("Mean Price")+
  ggtitle("Mean Price by Room Type")+
  geom_text(size = 3,position = position_stack(vjust = 0.5))+
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5))
```

Agregamos a nuestro modelo la interacción de estas dos variables *accomodates* y *room_type*.

```{r fit4, echo=FALSE}
fit_4 <- lm(log(price) ~ latitude +accommodates:room_type+minimum_nights+availability_365+reviews_per_month+host_exp_days, data = data_train)
summary(fit_4)
vif(fit_4)
```

```{r fit4 sum, echo=FALSE}
anova(fit_4)
plot(fit_4$residuals)
summary(gvlma(fit_4))
autoplot(fit_4)
```

Vemos que tampoco modelo sige las reglas básicas por que distribucion de lo de mas variables no es normal.


# Aplicación de técnicas automáticas de selección de variables

## Best subsets


El primer método que hemos utilizado es el de best subset selección. El resumen de todos los modelos examinados se muestra a continuación:

```{r rl regfit, echo=FALSE}
regfit_full <- leaps::regsubsets(log(price) ~ accommodates*room_type + neighbourhood_group_cleansed+minimum_nights + latitude + last_review_days+host_is_superhost+host_exp_days+review_scores_rating, data_train)

reg_sum <- summary(regfit_full)
reg_sum

```

Para evaluar los resultados obtenidos, se ha observado:
    - el coeficiente R2 ajustado asociado a cada modelo

```{r rl regfit sum, echo=FALSE}
reg_sum$adjr2
```
   
    - el criterio BIC asociado a cada modelo
```{r regfit sum1, echo=FALSE}
reg_sum$bic
```
   
    - el criterio CP asociado a cada modelo
    ```{r regfit sum2, echo=FALSE}
reg_sum$cp
```

```{r regfit sum3, echo=FALSE}
for (metric in c("r2", "adjr2", "Cp", "bic")){plot(regfit_full, scale=metric)}
```

Mejor opcion es modelo 8 con coeficientes:
```{r regfit sum4, echo=FALSE}
coef(regfit_full, 8) 
```

## Backward Selection

El último método de selección automática de variables que se ha puesto en práctica es el de Backward Selection. En este caso se parte de un modelo con todos los predictores y se van eliminando uno a uno (en cada paso se elimina la variable más significativa para el modelo):

```{r bwd, echo=FALSE}
regfit_bwd <- leaps::regsubsets(log(price) ~ accommodates*room_type + neighbourhood_group_cleansed+minimum_nights + latitude + last_review_days+host_is_superhost+host_exp_days+review_scores_rating, data_train, method="backward")

reg_sum_bwd <-summary(regfit_bwd)
reg_sum_bwd

```
 
 Para evaluar los resultados obtenidos, se ha observado:
    - el coeficiente R2 ajustado asociado a cada modelo

```{r bwd sum, echo=FALSE}
reg_sum_bwd$adjr2
```
   
    - el criterio BIC asociado a cada modelo
```{r bwd sum1, echo=FALSE}
reg_sum_bwd$bic
```
   
    - el criterio CP asociado a cada modelo
    ```{r bwd sum2, echo=FALSE}
reg_sum_bwd$cp
```

```{r bwd sum3, echo=FALSE}
for (metric in c("r2", "adjr2", "Cp", "bic")){plot(regfit_bwd, scale=metric)}
```

Mejor opcion es modelo 8 con coeficientes:
```{r bwd sum4, echo=FALSE}
coef(regfit_bwd, 8) 
```




## Forward Selection


Otra de las técnicas utilizadas es la de Forward Selection, en la que se parte de un modelo vacío y se van añadiendo variables. Los resultados obtenidos han sido los siguientes:

```{r fwd, echo=FALSE}
regfit_fwd <- leaps::regsubsets(log(price) ~ accommodates*room_type + neighbourhood_group_cleansed+minimum_nights + latitude + last_review_days+host_is_superhost+host_exp_days+review_scores_rating, data_train, method="forward")

reg_sum_fwd <-summary(regfit_fwd)
reg_sum_fwd

```
 
 Para evaluar los resultados obtenidos, se ha observado:
    - el coeficiente R2 ajustado asociado a cada modelo

```{r fwd sum, echo=FALSE}
reg_sum_fwd$adjr2
```
   
    - el criterio BIC asociado a cada modelo
```{r fwd sum1, echo=FALSE}
reg_sum_fwd$bic
```
   
    - el criterio CP asociado a cada modelo
    ```{r fwd sum2, echo=FALSE}
reg_sum_fwd$cp
```

```{r fwd sum3, echo=FALSE}
for (metric in c("r2", "adjr2", "Cp", "bic")){plot(regfit_fwd, scale=metric)}
```

Mejor opcion es modelo 8 con coeficientes:
```{r fwd sum4, echo=FALSE}
coef(regfit_fwd, 8) 
```

Se puede concluir, que las 3 técnicas de selección automática de variables indican que el número óptimo de predictores es 8. Además, según estas técnicas, las variables más significativas para el modelo serían:
      - accommodates
      - latitude
      - minimum_nights
      - room_type Private
      - numero de accommodates según el room_type Private
      - numero de accommodates según el room_type Shared
      - distrito Salamanca
      - distrito Centro


Comprobamos este modelo.
```{r fit_5, echo=FALSE}
x <- model.matrix(log(price)~ accommodates*room_type + neighbourhood_group_cleansed+minimum_nights + latitude + last_review_days+host_is_superhost+host_exp_days+review_scores_rating, data_train)[,-1]
y <- log(data_train$price) 
data_new <- data.frame(x)
data_new$price <- y
fit_5 <- lm(formula = y ~ accommodates + room_typePrivate + neighbourhood_group_cleansedCentro + neighbourhood_group_cleansedSalamanca + minimum_nights +  latitude + accommodates:room_typePrivate + accommodates:room_typeShared, data = data_new)
summary(fit_5)
anova(fit_5)
```

```{r fit_5 sum, echo=FALSE}
plot(fit_5$residuals)
summary(gvlma(fit_5))
autoplot(fit_5)

```

Vemos que tampoco modelo sige las reglas básicas por que distribucion de lo de mas variables no es normal.

# Técnicas de regularización

Además de las técnicas de selección de variables anteriores, se prueban técnicas de regularización. En lo siguiente, hay que tener en cuenta que con regularización Lasso se consigue que ciertos coeficientes se anulen, mientras que con Ridge se situarían muy cercanos a 0, pero sin anularse.

# Regularización Lasso

En la siguiente gráfica se muestra como varía el valor de los coeficientes para diferentes valores de lambda, hasta que finalmente se hacen todos 0 (esto con Ridge no ocurriría).

```{r Lasso 1, echo=FALSE}
grid <- 10^seq(10,-2,length=100)
lasso_reg <- glmnet(x, y, alpha = 1, lambda=grid)
plot(lasso_reg)
```

Para descubrir con qué valor de lambda se consigue el mejor modelo, y con cuántos predictores, el paquete ‘glmnet’ ofrece una función para averiguarlo. Para la evaluación de los modelos utiliza la técnica de validación cruzada y una función de pérdida que por defecto es el error cuadrático medio.

```{r Lasso 2, echo=FALSE}
set.seed(10)
cv_result <- cv.glmnet(x,y,alpha=1)
plot(cv_result)
```
En la gráfica podemos apreciar dos líneas de puntos. La primera marca el lambda para el cual se consigue el modelo con un error cuadrático medio más bajo. La segunda, marca el lambda para el cual se consigue el modelo más sencillo cuyo error se encuentra a 1 desviación estándar del mínimo.

En esta misma gráfica, podemos ver que se podría obtener un modelo con 17 predictores aproximadamente, cuyo error sería muy similar al del mejor modelo (que utiliza 32).

```{r Lasso sum, echo=FALSE}
cv_result
```

```{r Lasso sum1, echo=FALSE}
set.seed(10)
out <- glmnet(x,y,alpha=1,lambda = cv_result$lambda.1se)
lasso_coef <- predict(out,type="coefficients", s=cv_result$lambda.1se)[1:35,]
lasso_coef[lasso_coef!=0]
```

La información que obtenemos al aplicar regularización Lasso es similar a la que obteníamos con las técnicas aplicadas anteriormente. Se obtiene un SE de 0.002943  con 17 variables, y 0.002880 con 32.

Si se calculan los residuos de este modelo, se puede observar que no siguen una distribución normal.
```{r Lasso sum2, echo=FALSE}
preds_lasso <- predict(out,x)
residuals_lasso <- y - preds_lasso
lillie.test(residuals_lasso)
```

```{r Lasso sum3, echo=FALSE}
rsq_lasso <- cor(y, preds_lasso)^2
sprintf("R2 = %f", rsq_lasso)
```

MSE y MAPE de entrenamiento
```{r Lasso sum4, echo=FALSE}

training_mse <- mean((y-preds_lasso)^2)
paste("Error (mse) de entrenamiento:", training_mse)
training_mape <- mean((preds_lasso-y)/y)
paste("MAPE de entrenamiento:", training_mape)
```

## Regularización Ridge
Otra técnica de regularización que se ha aplicado es la regresión Ridge. En este caso, el penalty aplicado a los coeficientes reducirá los coeficientes menos importantes, pero nunca los hará completamente 0, independientemente de valor que tome lambda. Esto se puede comprobar en la siguiente gráfica:

```{r Ridge, echo=FALSE}
models_ridge <- glmnet(x = x, y = y, alpha = 0)
plot(models_ridge, xvar = "lambda", label = TRUE)
```

De igual forma que con Lasso, en la siguiente gráfica se puede observar el error cuadrático medio calculado mediante validación cruzada para diferentes valores de lambda:

```{r Ridge 1, echo=FALSE}
set.seed(10)
cv_ridge <- cv.glmnet(x = x, y = y, alpha = 0)
plot(cv_ridge)
```

En este caso, se muestran los coeficientes que se obtienen del modelo cuyo error se encuentra a 1 desviación estándar del mínimo (y donde empieza a incrementarse el error cuadrático medio):
```{r Ridge 2, echo=FALSE}
out_ridge <- glmnet(x,y,alpha=0,lambda = cv_ridge$lambda.1se)
ridge_coef <- predict(out_ridge,type="coefficients")[1:35,]
ridge_coef[ridge_coef!=0]
```

Con Ridge, se obtiene un SE de 0.002984 con 34 variables.
```{r Ridge 3, echo=FALSE}
cv_ridge
```

Si se calculan los residuos de este modelo, se puede observar que no siguen una distribución normal.

```{r Ridge 4, echo=FALSE}
preds_ridge <- predict(out_ridge,x)
residuals_ridge <- y - preds_ridge
lillie.test(residuals_ridge)
```

```{r Ridge sum3, echo=FALSE}
rsq_ridge <- cor(y, preds_ridge)^2
sprintf("R2 = %f", rsq_ridge)
```

MSE y MAPE de entrenamiento
```{r Ridge sum4, echo=FALSE}

training_mse <- mean((y-preds_ridge)^2)
paste("Error (mse) de entrenamiento:", training_mse)
training_mape <- mean((preds_ridge-y)/y)
paste("MAPE de entrenamiento:", training_mape)
```

# Regresion lineal usando otras variables y transformaciones

Como los objetos de alquiler más caros se centran cerca de Puerta del Sol, agregamos una nueva variable: la distancia a Puerta del Sol (*dist_sol*).
```{r sol, echo=FALSE}
data_train <- read_csv("data_train_clean.csv")

sol <- c(-3.7035799616333795, 40.417114256598694)
distance <- function(origen, row){
  return(as.numeric(distm(origen, cbind(row$longitude, row$latitude), 
                          fun = distHaversine))
  )
}
data_train$dist_sol <- distance(sol, data_train)
```

Ahora veamos el gráfico de la dependencia del precio de la distancia a Puerta del Sol usando *log(dist_sol)* (como es la mejor transformacion de acuerdo con methodo Box-Cox).

```{r sol gr, echo=FALSE}
BoxCoxTrans(data_train$dist_sol)

ggplot(data_train, aes(x = log(dist_sol), y = log(price))) +
  geom_point()+
  geom_smooth(method = 'lm')
```

Vemos que cuanto más lejos, más bajos son los precios en promedio. Considere esta dependencia de acuerdo con *room_type*.

```{r sol gr 2, echo=FALSE}
ggplot(data_train, aes(x = log(dist_sol), y = log(price), color=room_type)) +
  geom_point()+
  geom_smooth(method = 'lm')
```

El gráfico muestra que esta dependencia es diferente para diferentes tipos de alojamento.

También haremos la transformación logarítmica de la variable *accommodates* para revelar una relación lineal más clara entre las variables.

```{r accommodates gr, echo=FALSE}
BoxCoxTrans(data_train$accommodates)

ggplot(data_train, aes(x = log(accommodates), y = log(price))) +
  geom_point()+
  geom_smooth(method = 'lm')
```

Construyamos un modelo basado en estas dos dependencias
```{r model1, echo=FALSE}
fit_m1 <- lm(log(price) ~ log(accommodates)+room_type:log(dist_sol), data = data_train)
summary(gvlma(fit_m1))
```

Obviamente, el modelo tiene una relación lineal, pero los residuos no se distribuyen normalmente y no se cumplen otras condiciones.

```{r model1 summmm, echo=FALSE}
plot(fit_m1$residuals)
autoplot(fit_m1)
```

En el gráfico, podemos ver que todavía estamos lidiando con valores atípicos. Eleminamos valores atipicos.

```{r model1 outliers, echo=FALSE}
outliers.rm <- function(x){    
  q <- quantile(x, 0.25) + quantile(x, 0.75)    
  return(x[abs(x - q/2) <= 2*IQR(x)])}

#summary(outliers.rm(data_train$price_per_person))
#summary(outliers.rm(data_train$price))
#summary(outliers.rm(data_train$accommodates))
#summary(outliers.rm(data_train$minimum_nights))

data_train <- subset(data_train, price_per_person <= 56)
data_train <- subset(data_train, accommodates <= 7)
data_train <- subset(data_train, price<= 189)
data_train <- subset(data_train, minimum_nights<= 6)
```

Ahora obtenemos mejores resultados de modelo anterior.

```{r model2, echo=FALSE}
fit_m2 <- lm(log(price) ~ log(accommodates)+room_type:log(dist_sol), data = data_train)
summary(gvlma(fit_m2))
```

```{r model2 summmm, echo=FALSE}
plot(fit_m2$residuals)
autoplot(fit_m2)
```

Si se calculan los residuos de este modelo, se puede observar que no siguen una distribución normal, pero ya residuos estan mejorando.

```{r model2 summmm res, echo=FALSE}
lillie.test(fit_m2$residuals)
```

Agreguemos la interacción de variables (*log(reviews_per_month):log(dist_sol)*) al modelo.

```{r model3, echo=FALSE}
fit_m3 <- lm(log(price) ~ log(accommodates)+room_type:log(dist_sol)+log(reviews_per_month):log(dist_sol), data = data_train) #0.6552
summary(gvlma(fit_m3))
```

```{r model3 summmm, echo=FALSE}
plot(fit_m3$residuals)
autoplot(fit_m3)
```

Vemos que los parámetros del modelo han mejorado, los residuales se han vuelto más normales.

```{r model3 summmm res, echo=FALSE}
lillie.test(fit_m3$residuals)
```


